{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TradingMockUp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPi/QU/dcpWDPKj7qlnmEe3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jesusfigueres/surferbot/blob/main/TradingMockUp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SRS4IQnPG11"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "from datetime import datetime\r\n",
        "import itertools\r\n",
        "import argparse\r\n",
        "import re\r\n",
        "import os\r\n",
        "import pickle\r\n",
        "\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "\r\n",
        "#from trading_envs import BasicEnv\r\n",
        "#from trading_agents import AgentDQN,AgentDQN_imagination\r\n",
        "#from trading_utils import MLP,"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQPoAdIZol_7"
      },
      "source": [
        "class MLP(nn.Module):\r\n",
        "  def __init__(self, n_inputs, n_action, n_hidden_layers=1, hidden_dim=32):\r\n",
        "    super(MLP, self).__init__()\r\n",
        "\r\n",
        "    M = n_inputs\r\n",
        "    self.layers = []\r\n",
        "    for _ in range(n_hidden_layers):\r\n",
        "      layer = nn.Linear(M, hidden_dim)\r\n",
        "      M = hidden_dim\r\n",
        "      self.layers.append(layer)\r\n",
        "      self.layers.append(nn.ReLU())\r\n",
        "\r\n",
        "    # final layer\r\n",
        "    self.layers.append(nn.Linear(M, n_action))\r\n",
        "    self.layers = nn.Sequential(*self.layers)\r\n",
        "\r\n",
        "  def forward(self, X):\r\n",
        "    return self.layers(X)\r\n",
        "\r\n",
        "  def save_weights(self, path):\r\n",
        "    torch.save(self.state_dict(), path)\r\n",
        "\r\n",
        "  def load_weights(self, path):\r\n",
        "    self.load_state_dict(torch.load(path))\r\n",
        "\r\n",
        "\r\n",
        "#####################\r\n",
        "\r\n",
        "class ReplayBuffer:\r\n",
        "  def __init__(self, obs_dim, act_dim, size):\r\n",
        "    self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)\r\n",
        "    self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)\r\n",
        "    self.acts_buf = np.zeros(size, dtype=np.uint8)\r\n",
        "    self.rews_buf = np.zeros(size, dtype=np.float32)\r\n",
        "    self.done_buf = np.zeros(size, dtype=np.uint8)\r\n",
        "    self.ptr, self.size, self.max_size = 0, 0, size\r\n",
        "\r\n",
        "  def store(self, obs, act, rew, next_obs, done):\r\n",
        "    self.obs1_buf[self.ptr] = obs\r\n",
        "    self.obs2_buf[self.ptr] = next_obs\r\n",
        "    self.acts_buf[self.ptr] = act\r\n",
        "    self.rews_buf[self.ptr] = rew\r\n",
        "    self.done_buf[self.ptr] = done\r\n",
        "    self.ptr = (self.ptr+1) % self.max_size\r\n",
        "    self.size = min(self.size+1, self.max_size)\r\n",
        "\r\n",
        "  def sample_batch(self, batch_size=32):\r\n",
        "    idxs = np.random.randint(0, self.size, size=batch_size)\r\n",
        "    return dict(s=self.obs1_buf[idxs],\r\n",
        "                s2=self.obs2_buf[idxs],\r\n",
        "                a=self.acts_buf[idxs],\r\n",
        "                r=self.rews_buf[idxs],\r\n",
        "                d=self.done_buf[idxs])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJpvLCAwimju"
      },
      "source": [
        "# Environments\r\n",
        "######################\r\n",
        "\r\n",
        "class SimulationEnv():\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    return\r\n",
        "\r\n",
        "  def reset(self):\r\n",
        "    return\r\n",
        "\r\n",
        "  def step(self,action):\r\n",
        "    return next_state, reward, done, info\r\n",
        "\r\n",
        "  def pred_step(self,action):\r\n",
        "    return next_state, reward, done, info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsNHO8P-i1Nk"
      },
      "source": [
        "# Agents\r\n",
        "#######################\r\n",
        "\r\n",
        "class AgentDQN():\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "\r\n",
        "    return\r\n",
        "\r\n",
        "  def get_action(self,state):\r\n",
        "\r\n",
        "    return\r\n",
        "\r\n",
        "#######################\r\n",
        "\r\n",
        "class AgentDQN_im():\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "\r\n",
        "    return\r\n",
        "\r\n",
        "  def get_action(self,state):\r\n",
        "\r\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8jJ2o7QPIGk"
      },
      "source": [
        "# Create objects\r\n",
        "env = SimulationEnv()\r\n",
        "agent = AgentDQN()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XatVxRWcn_IK"
      },
      "source": [
        "def play_episode(env, agent):  \r\n",
        "  done = False\r\n",
        "  state = env.reset() # bring us back to init state\r\n",
        "\r\n",
        "  while not done:\r\n",
        "    action = agent.get_action(state) # could come from our agent\r\n",
        "\r\n",
        "    # perform the trade, 'info' contains portfolio value\r\n",
        "    next_state, reward, done, info = env.step(action)\r\n",
        "    \r\n",
        "    state = next_state\r\n",
        "\r\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}